#!/bin/bash
#SBATCH --job-name=stencil_mpi_single
#SBATCH --partition=GENOA
#SBATCH --mem=0G
#SBATCH --nodes=1                  # single node
#SBATCH --ntasks-per-node=2        # adjust if needed
#SBATCH --cpus-per-task=8          # 1 thread per NUMA region (8 NUMA domains on EPYC)
#SBATCH --time=00:01:00
#SBATCH --exclusive

# --- Task number detection ---
TASKS=$(( SLURM_NTASKS_PER_NODE * SLURM_JOB_NUM_NODES ))

# --- Configuration ---
EXEC=./stencil_parallel
ARGS="-x 25000 -y 25000 -n 200 -o 0"
RESULTS_FILE=mpi_single_run.log

# --- Environment Setup ---
module load openMPI/5.0.5

echo "--- Compiling the application ---"
make MODE=parallel LOG=0

# --- OpenMP Settings ---
export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}
export OMP_PLACES=cores
export OMP_PROC_BIND=spread
export OMP_DISPLAY_AFFINITY=TRUE

# --- Function to generate rankfile ---
# --- Function to generate a rankfile with custom task spreading ---
generate_rankfile() {
    local total_tasks=$1
    local tasks_per_node=$2
    local rankfile_name="rankfile_${total_tasks}.txt"

    echo "# Rankfile for $total_tasks tasks ($tasks_per_node per node) with custom spreading" > $rankfile_name

    NODELIST=$(scontrol show hostnames $SLURM_NODELIST)
    NODE_ARR=($NODELIST)

    local NUMA_DOMAINS=8
    local CORES_PER_NUMA=8

    # --- Define your custom spreading offsets here ---
    local offsets
    case "$tasks_per_node" in
        1)
            offsets=(0)
            ;;
        2)
            # Max spread: place tasks at the beginning and end of each CCD/NUMA domain
            offsets=(0 7)
            ;;
        3)
            # Spread out as requested
            offsets=(0 4 7)
            ;;
        4)
            # Spread evenly across the 8 cores of a CCD/NUMA domain
            offsets=(0 2 4 6)
            ;;
        *)
            echo "ERROR: Unsupported number of tasks per node for custom spreading: $tasks_per_node"
            echo "Please define a mapping for it in the script's case statement."
            exit 1
            ;;
    esac

    rank=0
    for ((task=0; task<total_tasks; task++)); do
        node_index=$(( task / tasks_per_node ))
        hostname=${NODE_ARR[$node_index]}

        # This is the task's index *on its specific node* (0, 1, 2, etc.)
        local local_task_idx=$(( task % tasks_per_node ))
        
        # Get the custom starting core from our predefined list
        local offset=${offsets[$local_task_idx]}

        core_list=""
        for ((numa=0; numa<NUMA_DOMAINS; numa++)); do
            core=$((numa * CORES_PER_NUMA + offset))
            core_list="${core_list:+$core_list,}$core"
        done

        echo "rank $rank=${hostname} slot=$core_list" >> $rankfile_name
        rank=$((rank+1))
    done

    echo $rankfile_name
}

# --- Results File Header ---
echo "Tasks,TotalTime,MaxCompTime,MaxCommTime,EnergyCompTime" > $RESULTS_FILE

echo ">>> Running with $TASKS MPI tasks, ${OMP_NUM_THREADS} threads per task"

RANKFILE=$(generate_rankfile $TASKS)
echo "Generated rankfile: $RANKFILE"
cat $RANKFILE
echo "---"

OUTPUT=$(mktemp)

# --- MPI Execution ---
mpirun -np $TASKS \
       --map-by rankfile:file=$RANKFILE \
       --bind-to core \
       --display-map \
       --report-bindings \
       $EXEC $ARGS > $OUTPUT

# --- Data Extraction ---
TOTAL_TIME=$(grep "Total time" $OUTPUT | awk '{print $3}')
MAX_COMP=$(grep "Max computation time" $OUTPUT | awk '{print $6}')
MAX_COMM=$(grep "Max communication time" $OUTPUT | awk '{print $6}')
ENERGY_TIME=$(grep "Total energy computaton time" $OUTPUT | awk '{print $5}')

# --- Save Results ---
if [ -n "$TOTAL_TIME" ]; then
    echo "$TASKS,$TOTAL_TIME,$MAX_COMP,$MAX_COMM,$ENERGY_TIME" >> $RESULTS_FILE
    echo "Results: Tasks=$TASKS, Total=$TOTAL_TIME, Comp=$MAX_COMP, Comm=$MAX_COMM, Energy=$ENERGY_TIME"
else
    echo "$TASKS,ERROR,ERROR,ERROR,ERROR" >> $RESULTS_FILE
    echo "Error parsing output. Check temporary output file: $OUTPUT"
fi

rm $OUTPUT
rm -f rankfile_*.txt

echo "Single run complete. Results are in $RESULTS_FILE"
